{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/models/mreso/torchtitan\n"
          ]
        }
      ],
      "source": [
        "%cd /mnt/models/mreso/torchtitan/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663e41ce",
      "metadata": {},
      "source": [
        "### 2. Define your Titan and cluster parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "39d51df7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
        "\n",
        "from torchtitan.train import Trainer\n",
        "from torchtitan.config import ConfigManager, JobConfig\n",
        "from monarch.actor import Actor, current_rank, endpoint\n",
        "from torchtitan.tools.logging import init_logger, logger\n",
        "import torch.distributed as dist\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "from monarch.tools import commands\n",
        "from monarch.utils import setup_env_for_distributed\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RunParams:\n",
        "    \"\"\"\n",
        "        Parameters for your cluster and training job, adjust as needed\n",
        "    \"\"\"\n",
        "    training_steps: int = 50\n",
        "    model_config = \"/mnt/models/mreso/torchtitan/torchtitan/models/llama3/train_configs/debug_model.toml\"\n",
        "    dataset = \"c4_test\"\n",
        "    num_nodes = 1\n",
        "    gpus_per_node = 1\n",
        "\n",
        "\n",
        "class TrainerActor():\n",
        "    \"\"\"\n",
        "        A simple wrapper class with executes a TorchTitan trainer in a Monarch actor\n",
        "    \"\"\"\n",
        "    def __init__(self, job_config: JobConfig) -> None:\n",
        "        self.job_config = job_config\n",
        "        rank = 0\n",
        "        self.uid = f\"[trainer_{rank}]\"\n",
        "\n",
        "    def start_training(self) -> None:\n",
        "        init_logger()\n",
        "        trainer: Trainer | None = None\n",
        "\n",
        "        try:\n",
        "            trainer = Trainer(self.job_config)\n",
        "            logger.info(f\"{self.uid} initialized successfully and starting training\")\n",
        "            trainer.train()\n",
        "        except Exception:\n",
        "            if trainer:\n",
        "                trainer.close()\n",
        "            raise\n",
        "        else:\n",
        "            trainer.close()\n",
        "        finally:\n",
        "            torch.distributed.destroy_process_group()\n",
        "            logger.info(f\"{self.uid} trainer cleaned up\")\n",
        "\n",
        "def make_job_config() -> JobConfig:\n",
        "    \"\"\"\n",
        "        Create a job config which is digested by TorchTitan, sourced from RunParams\n",
        "    \"\"\"\n",
        "    data_parallel_shard_degree = RunParams.num_nodes * RunParams.gpus_per_node\n",
        "    output_path = \"./outputs\"\n",
        "\n",
        "    script_dir = globals()['_dh'][0]\n",
        "    default_args = [\n",
        "        \"--job.config_file\",\n",
        "        os.path.join(script_dir, RunParams.model_config),\n",
        "        \"--model.tokenizer_path\",\n",
        "        os.path.join(script_dir, \"examples/assets/hf/Llama-3.1-8B/\"),\n",
        "        \"--comm.trace_buf_size\",\n",
        "        \"0\",\n",
        "        \"--metrics.log_freq\",\n",
        "        \"1\",\n",
        "        \"--parallelism.data_parallel_shard_degree\",\n",
        "        str(data_parallel_shard_degree),\n",
        "        \"--activation_checkpoint.mode\",\n",
        "        \"full\",\n",
        "        \"--comm.train_timeout_seconds\",\n",
        "        \"60\",\n",
        "        \"--training.steps\",\n",
        "        str(RunParams.training_steps),\n",
        "        \"--training.dataset\",\n",
        "        RunParams.dataset,\n",
        "        \"--job.dump_folder\",\n",
        "        output_path,\n",
        "        \"--metrics.enable_tensorboard\",\n",
        "    ]\n",
        "\n",
        "    config_manager = ConfigManager()\n",
        "    job_config = config_manager.parse_args(default_args)\n",
        "\n",
        "    return job_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "env = {\n",
        "    \"MASTER_ADDR\": \"localhost\",\n",
        "    \"MASTER_PORT\": str(27000),\n",
        "    \"RANK\": str(0),\n",
        "    \"LOCAL_RANK\": str(0),\n",
        "    \"LOCAL_WORLD_SIZE\": str(1),\n",
        "    \"WORLD_SIZE\": str(1),\n",
        "}\n",
        "\n",
        "os.environ.update(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[titan] 2025-10-02 01:09:17,333 - root - INFO - Starting job: Llama 3 debug training\n",
            "[titan] 2025-10-02 01:09:17,335 - root - INFO - Building 0-D device mesh with [], []\n",
            "[titan] 2025-10-02 01:09:17,359 - root - INFO - [GC] Initial GC collection took 0.00 seconds\n",
            "[titan] 2025-10-02 01:09:17,728 - root - INFO - Loading tokenizer from tokenizer.json\n",
            "[titan] 2025-10-02 01:09:17,873 - root - INFO - Preparing c4_test dataset from tests/assets/c4_test\n",
            "[titan] 2025-10-02 01:09:17,891 - root - INFO - Building llama3 debugmodel with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=256, n_layers=6, n_heads=16, n_kv_heads=None, vocab_size=2048, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)\n",
            "[titan] 2025-10-02 01:09:17,897 - root - INFO - TensorBoard logging enabled. Logs will be saved at ./outputs/tb/20251002-0109\n",
            "[titan] 2025-10-02 01:09:17,898 - root - INFO - CUDA capacity: AMD Instinct MI325X with 255.98GiB memory\n",
            "[titan] 2025-10-02 01:09:17,927 - root - INFO - \u001b[34mModel llama3 debugmodel \u001b[31msize: 6,163,712 total parameters\u001b[39m\n",
            "[titan] 2025-10-02 01:09:17,928 - root - INFO - Applied full activation checkpointing to the model\n",
            "[titan] 2025-10-02 01:09:18,093 - root - INFO - Peak FLOPS used for computing MFU: 1.300e+15\n",
            "[titan] 2025-10-02 01:09:18,094 - root - INFO - CUDA memory usage for model: 0.04GiB(0.02%)\n",
            "[titan] 2025-10-02 01:09:18,095 - root - WARNING - model.safetensors.index.json not found at hf_assets_path: /mnt/models/mreso/monarch/examples/assets/hf/Llama-3.1-8B/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format\n",
            "[titan] 2025-10-02 01:09:18,096 - root - INFO - Mixed precision training is handled by AMP\n",
            "[titan] 2025-10-02 01:09:18,096 - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 50 (warmup 2)\n",
            "[titan] 2025-10-02 01:09:18,096 - root - INFO - [trainer_0] initialized successfully and starting training\n",
            "[titan] 2025-10-02 01:09:18,096 - root - INFO - Training starts at step 1\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "trainer = TrainerActor(make_job_config())\n",
        "\n",
        "print(f\"{torch.__version__}\")\n",
        "\n",
        "# trainer.start_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "2fa680ca-06ba-41e3-ac40-90b22d77bbc3",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
